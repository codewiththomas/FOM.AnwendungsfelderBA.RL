{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83e\udd16 Reinforcement Learning f\u00fcr optimale Dienstplanung Dieses Projekt implementiert ein Single-Agent Reinforcement Learning System zur optimalen Dienstplanung im Call-Center-Umfeld. Der RL-Agent lernt, Mitarbeiter optimal zu Zeitslots zuzuweisen und dabei alle relevanten Constraints zu ber\u00fccksichtigen. \ud83c\udfaf Projektziele Service Level erreichen : 80/20 Service Level (80% der Anrufe innerhalb 20 Sekunden) Constraints einhalten : Arbeitszeiten, Pausen, Ruhezeiten Kosten optimieren : Minimale \u00dcberbesetzung bei maximaler Effizienz Flexibilit\u00e4t : Automatische Anpassung an Forecast-\u00c4nderungen \ud83c\udfd7\ufe0f Projektstruktur \u251c\u2500\u2500 data_csv/ # CSV-Daten f\u00fcr Training \u2502 \u251c\u2500\u2500 agents.csv # Mitarbeiter-Daten \u2502 \u251c\u2500\u2500 lines.csv # Line-Informationen (AHT) \u2502 \u251c\u2500\u2500 forecast.csv # Kontakt-Forecast \u2502 \u2514\u2500\u2500 constraints.csv # Constraint-Definitionen \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 environment/ \u2502 \u2502 \u2514\u2500\u2500 shift_scheduling_env.py # Gymnasium Environment \u2502 \u251c\u2500\u2500 models/ \u2502 \u2502 \u2514\u2500\u2500 train_agent.py # Training mit Stable-Baselines3 \u2502 \u251c\u2500\u2500 utils/ \u2502 \u2502 \u2514\u2500\u2500 data_loader.py # CSV-Datenverarbeitung \u2502 \u2514\u2500\u2500 evaluation/ # Evaluation und Metriken \u251c\u2500\u2500 requirements.txt # Python-Dependencies \u251c\u2500\u2500 test_environment.py # Environment-Tests \u2514\u2500\u2500 train_shift_scheduler.py # Hauptskript f\u00fcr Training \ud83d\ude80 Quick Start 1. Installation # Python Virtual Environment erstellen python -m venv .venv source .venv/bin/activate # Windows: rl_scheduling_env\\Scripts\\activate # Dependencies installieren pip install -r requirements.txt 2. Environment testen # Basis-Tests ohne RL-Training python train_shift_scheduler.py --mode test 3. RL-Training starten # Schnelles Training (50k steps) python train_shift_scheduler.py --mode train --timesteps 50000 # Vollst\u00e4ndiges Training (200k steps) python train_shift_scheduler.py --mode train --timesteps 200000 --max_steps 500 4. Trainiertes Model evaluieren python train_shift_scheduler.py --mode eval --model_path results/models/best_model \ud83d\udcca Datenstruktur Agents (agents.csv) csvId,DisplayName,Wochenstunden,Quali_Line1,Quali_Line2,Quali_Line3 A001,Mueller_Max,40.0,True,False,False Forecast (forecast.csv) csvDatum,Interval,Erwartete_Kontakte 2025-06-09,07:00-07:30,12 Constraints (constraints.csv) csvID,Titel,Schweregrad,IsPositiv,Gewicht MIN_REST_TIME,Mindest-Ruhezeit zwischen zwei Schichten,Muss,-1,100 \ud83e\udd16 RL-Environment Details State Space Schedule Matrix : Aktuelle Zuweisungen (10 Agenten \u00d7 210 Zeitslots) Forecast : Erwartete Kontakte pro Zeitslot Agent Verf\u00fcgbarkeit : Wochenstunden und Qualifikationen Constraint Status : Aktuelle Violations und Compliance Action Space Discrete Actions : Agent_Index * Time_Slots + Time_Slot Toggle-Mechanismus : Zuweisen (0\u21921) oder Entfernen (1\u21920) Action Space Size : 2,100 (10 Agenten \u00d7 210 Zeitslots) Reward Function reward = service_level_reward + efficiency_bonus - constraint_penalties + hours_compliance Komponenten: - Service Level : +2.0 wenn \u226580%, -1.0 \u00d7 (0.8 - level) wenn <80% - Constraint Penalties : -0.5 pro Violation - Efficiency : +0.5 bei optimaler Agenten-Zuordnung - Hours Compliance : +0.3 bei korrekter Wochenstunden-Einhaltung \ud83d\udcc8 Training-Algorithmus PPO (Proximal Policy Optimization) - Policy : Multi-Layer Perceptron (MLP) - Learning Rate : 3e-4 - Batch Size : 64 - Gamma : 0.99 - Training Steps : 50k - 200k Training-Parameter training_params = { 'total_timesteps': 100000, 'max_steps': 300, 'learning_rate': 3e-4, 'batch_size': 64, 'gamma': 0.99 } \ud83c\udfaf Evaluation-Metriken Prim\u00e4re Metriken Service Level : % Zeitslots mit ausreichend Agenten Constraint Violations : Anzahl verletzter Constraints Hours Compliance : Einhaltung der Wochenstunden Schedule Completeness : Vollst\u00e4ndigkeit der Planung Constraint-Typen Muss-Constraints (Gewicht: 75-100) Mindest-/Maximal-Schichtl\u00e4nge Ruhezeiten zwischen Schichten Obligatorische Pausen Soll-Constraints (Gewicht: 50-70) Service Level 80/20 Wochenstunden-Einhaltung Wochenend-Abdeckung Kann-Constraints (Gewicht: 30-40) Kostenoptimierung Gleichm\u00e4\u00dfige Verteilung \ud83d\udcca Ergebnisse Das System wird in results/ gespeichert: results/ \u251c\u2500\u2500 models/ \u2502 \u251c\u2500\u2500 best_model.zip # Bestes Model w\u00e4hrend Training \u2502 \u2514\u2500\u2500 final_model.zip # Finales trainiertes Model \u251c\u2500\u2500 logs/ \u2502 \u251c\u2500\u2500 training_log.csv # Training-Verlauf \u2502 \u2514\u2500\u2500 tensorboard/ # TensorBoard-Logs \u251c\u2500\u2500 plots/ \u2502 \u2514\u2500\u2500 training_results.png # Visualisierungen \u251c\u2500\u2500 evaluation_results.csv # Evaluation-Statistiken \u2514\u2500\u2500 final_schedule.csv # Optimaler Schedule \ud83d\udee0\ufe0f Erweiterte Nutzung Custom Training from src.models.train_agent import ShiftSchedulingTrainer trainer = ShiftSchedulingTrainer(data_path=\"data_csv\") trainer.train_model( total_timesteps=100000, learning_rate=1e-3, batch_size=128 ) Environment Anpassungen from src.environment.shift_scheduling_env import ShiftSchedulingEnv env = ShiftSchedulingEnv( data_path=\"custom_data/\", max_steps=500 ) \ud83d\udd27 Troubleshooting H\u00e4ufige Probleme Import-Fehler bash pip install -r requirements.txt CSV-Dateien fehlen Pr\u00fcfen Sie den data_csv/ Ordner Stellen Sie sicher, dass alle 4 CSV-Dateien vorhanden sind GPU/CPU Performance python # F\u00fcr CPU-only Training import torch torch.device('cpu') Memory Issues Reduzieren Sie max_steps Parameter Verwenden Sie kleinere batch_size Debug-Modus # Ausf\u00fchrliche Logs python train_shift_scheduler.py --mode train --timesteps 10000 --verbose 2 # Environment-Details python test_environment.py \ud83d\udcda Technische Details Dependencies stable-baselines3 : RL-Algorithmen gymnasium : Environment-Framework pandas/numpy : Datenverarbeitung matplotlib/seaborn : Visualisierung torch : Deep Learning Backend Performance Training Zeit : 30-60 Min (50k steps) Memory Usage : ~1-2 GB RAM GPU : Optional, aber empfohlen f\u00fcr >100k steps \ud83d\udd2e Roadmap Geplante Features [ ] Multi-Agent RL : Jeder Agent als eigenst\u00e4ndiger RL-Agent [ ] Hierarchical RL : Schicht-Level und Agent-Level Entscheidungen [ ] Transfer Learning : Pre-trained Models f\u00fcr neue Szenarien [ ] Real-time Adaptation : Online-Learning bei Forecast-\u00c4nderungen [ ] Advanced Constraints : Qualifikations-Matching, Teamwork [ ] Web Interface : Dashboard f\u00fcr Schedule-Visualisierung Algorithmus-Verbesserungen [ ] SAC : Soft Actor-Critic f\u00fcr bessere Exploration [ ] Rainbow DQN : Erweiterte DQN-Varianten [ ] Meta-Learning : Schnelle Anpassung an neue Szenarien \ud83e\udd1d Contributing Fork das Repository Erstellen Sie einen Feature-Branch Implementieren Sie Ihre \u00c4nderungen F\u00fcgen Sie Tests hinzu Erstellen Sie einen Pull Request \ud83d\udcc4 Lizenz Dieses Projekt steht unter der MIT-Lizenz. Siehe LICENSE f\u00fcr Details. \ud83d\udcde Support Bei Fragen oder Problemen: - Erstellen Sie ein Issue im Repository - \u00dcberpr\u00fcfen Sie die Troubleshooting-Sektion - Kontaktieren Sie das Entwicklerteam Hinweis : Dieses Projekt dient als Proof-of-Concept f\u00fcr RL-basierte Dienstplanung. F\u00fcr Produktionseinsatz sind zus\u00e4tzliche Validierungen und Tests erforderlich.","title":"\ud83e\udd16 Reinforcement Learning f\u00fcr optimale Dienstplanung"},{"location":"#reinforcement-learning-fur-optimale-dienstplanung","text":"Dieses Projekt implementiert ein Single-Agent Reinforcement Learning System zur optimalen Dienstplanung im Call-Center-Umfeld. Der RL-Agent lernt, Mitarbeiter optimal zu Zeitslots zuzuweisen und dabei alle relevanten Constraints zu ber\u00fccksichtigen.","title":"\ud83e\udd16 Reinforcement Learning f\u00fcr optimale Dienstplanung"},{"location":"#projektziele","text":"Service Level erreichen : 80/20 Service Level (80% der Anrufe innerhalb 20 Sekunden) Constraints einhalten : Arbeitszeiten, Pausen, Ruhezeiten Kosten optimieren : Minimale \u00dcberbesetzung bei maximaler Effizienz Flexibilit\u00e4t : Automatische Anpassung an Forecast-\u00c4nderungen","title":"\ud83c\udfaf Projektziele"},{"location":"#projektstruktur","text":"\u251c\u2500\u2500 data_csv/ # CSV-Daten f\u00fcr Training \u2502 \u251c\u2500\u2500 agents.csv # Mitarbeiter-Daten \u2502 \u251c\u2500\u2500 lines.csv # Line-Informationen (AHT) \u2502 \u251c\u2500\u2500 forecast.csv # Kontakt-Forecast \u2502 \u2514\u2500\u2500 constraints.csv # Constraint-Definitionen \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 environment/ \u2502 \u2502 \u2514\u2500\u2500 shift_scheduling_env.py # Gymnasium Environment \u2502 \u251c\u2500\u2500 models/ \u2502 \u2502 \u2514\u2500\u2500 train_agent.py # Training mit Stable-Baselines3 \u2502 \u251c\u2500\u2500 utils/ \u2502 \u2502 \u2514\u2500\u2500 data_loader.py # CSV-Datenverarbeitung \u2502 \u2514\u2500\u2500 evaluation/ # Evaluation und Metriken \u251c\u2500\u2500 requirements.txt # Python-Dependencies \u251c\u2500\u2500 test_environment.py # Environment-Tests \u2514\u2500\u2500 train_shift_scheduler.py # Hauptskript f\u00fcr Training","title":"\ud83c\udfd7\ufe0f Projektstruktur"},{"location":"#quick-start","text":"","title":"\ud83d\ude80 Quick Start"},{"location":"#1-installation","text":"# Python Virtual Environment erstellen python -m venv .venv source .venv/bin/activate # Windows: rl_scheduling_env\\Scripts\\activate # Dependencies installieren pip install -r requirements.txt","title":"1. Installation"},{"location":"#2-environment-testen","text":"# Basis-Tests ohne RL-Training python train_shift_scheduler.py --mode test","title":"2. Environment testen"},{"location":"#3-rl-training-starten","text":"# Schnelles Training (50k steps) python train_shift_scheduler.py --mode train --timesteps 50000 # Vollst\u00e4ndiges Training (200k steps) python train_shift_scheduler.py --mode train --timesteps 200000 --max_steps 500","title":"3. RL-Training starten"},{"location":"#4-trainiertes-model-evaluieren","text":"python train_shift_scheduler.py --mode eval --model_path results/models/best_model","title":"4. Trainiertes Model evaluieren"},{"location":"#datenstruktur","text":"","title":"\ud83d\udcca Datenstruktur"},{"location":"#agents-agentscsv","text":"csvId,DisplayName,Wochenstunden,Quali_Line1,Quali_Line2,Quali_Line3 A001,Mueller_Max,40.0,True,False,False","title":"Agents (agents.csv)"},{"location":"#forecast-forecastcsv","text":"csvDatum,Interval,Erwartete_Kontakte 2025-06-09,07:00-07:30,12","title":"Forecast (forecast.csv)"},{"location":"#constraints-constraintscsv","text":"csvID,Titel,Schweregrad,IsPositiv,Gewicht MIN_REST_TIME,Mindest-Ruhezeit zwischen zwei Schichten,Muss,-1,100","title":"Constraints (constraints.csv)"},{"location":"#rl-environment-details","text":"","title":"\ud83e\udd16 RL-Environment Details"},{"location":"#state-space","text":"Schedule Matrix : Aktuelle Zuweisungen (10 Agenten \u00d7 210 Zeitslots) Forecast : Erwartete Kontakte pro Zeitslot Agent Verf\u00fcgbarkeit : Wochenstunden und Qualifikationen Constraint Status : Aktuelle Violations und Compliance","title":"State Space"},{"location":"#action-space","text":"Discrete Actions : Agent_Index * Time_Slots + Time_Slot Toggle-Mechanismus : Zuweisen (0\u21921) oder Entfernen (1\u21920) Action Space Size : 2,100 (10 Agenten \u00d7 210 Zeitslots)","title":"Action Space"},{"location":"#reward-function","text":"reward = service_level_reward + efficiency_bonus - constraint_penalties + hours_compliance Komponenten: - Service Level : +2.0 wenn \u226580%, -1.0 \u00d7 (0.8 - level) wenn <80% - Constraint Penalties : -0.5 pro Violation - Efficiency : +0.5 bei optimaler Agenten-Zuordnung - Hours Compliance : +0.3 bei korrekter Wochenstunden-Einhaltung","title":"Reward Function"},{"location":"#training-algorithmus","text":"PPO (Proximal Policy Optimization) - Policy : Multi-Layer Perceptron (MLP) - Learning Rate : 3e-4 - Batch Size : 64 - Gamma : 0.99 - Training Steps : 50k - 200k","title":"\ud83d\udcc8 Training-Algorithmus"},{"location":"#training-parameter","text":"training_params = { 'total_timesteps': 100000, 'max_steps': 300, 'learning_rate': 3e-4, 'batch_size': 64, 'gamma': 0.99 }","title":"Training-Parameter"},{"location":"#evaluation-metriken","text":"","title":"\ud83c\udfaf Evaluation-Metriken"},{"location":"#primare-metriken","text":"Service Level : % Zeitslots mit ausreichend Agenten Constraint Violations : Anzahl verletzter Constraints Hours Compliance : Einhaltung der Wochenstunden Schedule Completeness : Vollst\u00e4ndigkeit der Planung","title":"Prim\u00e4re Metriken"},{"location":"#constraint-typen","text":"Muss-Constraints (Gewicht: 75-100) Mindest-/Maximal-Schichtl\u00e4nge Ruhezeiten zwischen Schichten Obligatorische Pausen Soll-Constraints (Gewicht: 50-70) Service Level 80/20 Wochenstunden-Einhaltung Wochenend-Abdeckung Kann-Constraints (Gewicht: 30-40) Kostenoptimierung Gleichm\u00e4\u00dfige Verteilung","title":"Constraint-Typen"},{"location":"#ergebnisse","text":"Das System wird in results/ gespeichert: results/ \u251c\u2500\u2500 models/ \u2502 \u251c\u2500\u2500 best_model.zip # Bestes Model w\u00e4hrend Training \u2502 \u2514\u2500\u2500 final_model.zip # Finales trainiertes Model \u251c\u2500\u2500 logs/ \u2502 \u251c\u2500\u2500 training_log.csv # Training-Verlauf \u2502 \u2514\u2500\u2500 tensorboard/ # TensorBoard-Logs \u251c\u2500\u2500 plots/ \u2502 \u2514\u2500\u2500 training_results.png # Visualisierungen \u251c\u2500\u2500 evaluation_results.csv # Evaluation-Statistiken \u2514\u2500\u2500 final_schedule.csv # Optimaler Schedule","title":"\ud83d\udcca Ergebnisse"},{"location":"#erweiterte-nutzung","text":"","title":"\ud83d\udee0\ufe0f Erweiterte Nutzung"},{"location":"#custom-training","text":"from src.models.train_agent import ShiftSchedulingTrainer trainer = ShiftSchedulingTrainer(data_path=\"data_csv\") trainer.train_model( total_timesteps=100000, learning_rate=1e-3, batch_size=128 )","title":"Custom Training"},{"location":"#environment-anpassungen","text":"from src.environment.shift_scheduling_env import ShiftSchedulingEnv env = ShiftSchedulingEnv( data_path=\"custom_data/\", max_steps=500 )","title":"Environment Anpassungen"},{"location":"#troubleshooting","text":"","title":"\ud83d\udd27 Troubleshooting"},{"location":"#haufige-probleme","text":"Import-Fehler bash pip install -r requirements.txt CSV-Dateien fehlen Pr\u00fcfen Sie den data_csv/ Ordner Stellen Sie sicher, dass alle 4 CSV-Dateien vorhanden sind GPU/CPU Performance python # F\u00fcr CPU-only Training import torch torch.device('cpu') Memory Issues Reduzieren Sie max_steps Parameter Verwenden Sie kleinere batch_size","title":"H\u00e4ufige Probleme"},{"location":"#debug-modus","text":"# Ausf\u00fchrliche Logs python train_shift_scheduler.py --mode train --timesteps 10000 --verbose 2 # Environment-Details python test_environment.py","title":"Debug-Modus"},{"location":"#technische-details","text":"","title":"\ud83d\udcda Technische Details"},{"location":"#dependencies","text":"stable-baselines3 : RL-Algorithmen gymnasium : Environment-Framework pandas/numpy : Datenverarbeitung matplotlib/seaborn : Visualisierung torch : Deep Learning Backend","title":"Dependencies"},{"location":"#performance","text":"Training Zeit : 30-60 Min (50k steps) Memory Usage : ~1-2 GB RAM GPU : Optional, aber empfohlen f\u00fcr >100k steps","title":"Performance"},{"location":"#roadmap","text":"","title":"\ud83d\udd2e Roadmap"},{"location":"#geplante-features","text":"[ ] Multi-Agent RL : Jeder Agent als eigenst\u00e4ndiger RL-Agent [ ] Hierarchical RL : Schicht-Level und Agent-Level Entscheidungen [ ] Transfer Learning : Pre-trained Models f\u00fcr neue Szenarien [ ] Real-time Adaptation : Online-Learning bei Forecast-\u00c4nderungen [ ] Advanced Constraints : Qualifikations-Matching, Teamwork [ ] Web Interface : Dashboard f\u00fcr Schedule-Visualisierung","title":"Geplante Features"},{"location":"#algorithmus-verbesserungen","text":"[ ] SAC : Soft Actor-Critic f\u00fcr bessere Exploration [ ] Rainbow DQN : Erweiterte DQN-Varianten [ ] Meta-Learning : Schnelle Anpassung an neue Szenarien","title":"Algorithmus-Verbesserungen"},{"location":"#contributing","text":"Fork das Repository Erstellen Sie einen Feature-Branch Implementieren Sie Ihre \u00c4nderungen F\u00fcgen Sie Tests hinzu Erstellen Sie einen Pull Request","title":"\ud83e\udd1d Contributing"},{"location":"#lizenz","text":"Dieses Projekt steht unter der MIT-Lizenz. Siehe LICENSE f\u00fcr Details.","title":"\ud83d\udcc4 Lizenz"},{"location":"#support","text":"Bei Fragen oder Problemen: - Erstellen Sie ein Issue im Repository - \u00dcberpr\u00fcfen Sie die Troubleshooting-Sektion - Kontaktieren Sie das Entwicklerteam Hinweis : Dieses Projekt dient als Proof-of-Concept f\u00fcr RL-basierte Dienstplanung. F\u00fcr Produktionseinsatz sind zus\u00e4tzliche Validierungen und Tests erforderlich.","title":"\ud83d\udcde Support"},{"location":"installation/","text":"Installation GitHub Repository klonen https://github.com/codewiththomas/FOM.AnwendungsfelderBA.RL.git Python Virtual Environment erstellen py -3.12 -m venv .venv Das Environment aktivieren #Unix / MAX .venv/bin/activate #Windows .venv\\Scripts\\activate","title":"Installation"},{"location":"installation/#installation","text":"GitHub Repository klonen https://github.com/codewiththomas/FOM.AnwendungsfelderBA.RL.git Python Virtual Environment erstellen py -3.12 -m venv .venv Das Environment aktivieren #Unix / MAX .venv/bin/activate #Windows .venv\\Scripts\\activate","title":"Installation"},{"location":"mkdocs/","text":"","title":"Mkdocs"}]}