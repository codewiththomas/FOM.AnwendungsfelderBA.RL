## 6.5 Analyse der Lernkonvergenz

Die Analyse des Trainingsverlaufs über unterschiedliche Episodenzahlen hinweg zeigt charakteristische Muster der Q-Learning-Konvergenz, die jedoch stark szenario-abhängig variieren. Die empirischen Ergebnisse aus den untersuchten Szenarien A und B Alternative demonstrieren sowohl erfolgreiche Konvergenzpfade als auch strukturelle Herausforderungen im Reinforcement Learning-Ansatz und verdeutlichen die kritische Bedeutung des Problem-Designs für den Algorithmus-Erfolg.

Das Training in Szenario B Alternative über 1.000 Episoden zeigt die theoretisch erwartete Lernkurve eines erfolgreichen Q-Learning-Prozesses. In den ersten 300 Episoden dominiert erwartungsgemäß die Exploration, was zu volatilen Reward-Werten zwischen 800 und 1.000 Punkten und inkonsistenten Planungsstrategien führt. Diese initiale Volatilität ist charakteristisch für die Erkundungsphase, in der der Agent verschiedene Aktionen testet und deren Auswirkungen auf die Gesamtperformance evaluiert. Ab Episode 600 stabilisiert sich die Performance zunehmend, wobei der kumulierte Reward einen asymptotischen Verlauf zu einem finalen Durchschnittswert von 964,20 ± 33,85 Punkten in den letzten 100 Episoden zeigt.

Die Konvergenzanalyse verdeutlicht die progressive Verbesserung der Planungsqualität während des Trainings. Besonders bemerkenswert ist die Reduktion der Standardabweichung von 35,37 auf 34,62 Punkte, was einer Verbesserung von 2,1% entspricht und eine zunehmende Konsistenz der generierten Pläne anzeigt. Der berechnete Stabilitätskoeffizient von 0,036 liegt dabei im optimalen Bereich für produktive RL-Systeme und bestätigt die erfolgreiche Konvergenz. Nach 1.000 Episoden erreicht das Modell eine stabile Performance von 96,4% der maximal erreichten Reward-Rate von 1.045 Punkten, wobei weitere Trainingsepisoden nur noch marginale Verbesserungen erzielen würden.

Im deutlichen Kontrast dazu offenbart Szenario A über einen erweiterten Trainingszeitraum von 2.000 Episoden strukturelle Probleme in der Lernkonvergenz. Die durchschnittliche Performance verschlechtert sich kontinuierlich von initial -4.784 auf -6.544 Punkte in den letzten 100 Episoden, was auf eine problematische Belohnungsstruktur oder übermäßig restriktive Constraint-Definitionen hindeutet. Diese negative Performance-Entwicklung steht im direkten Widerspruch zu den theoretischen Erwartungen an Q-Learning-Algorithmen und deutet auf fundamentale Probleme in der Problemmodellierung hin.

Trotz der negativen Performance-Entwicklung in Szenario A zeigt sich dennoch eine signifikante Variabilitätsreduktion von 48,6%, bei der die Standardabweichung von 944,15 auf 485,18 Punkte sinkt. Diese Beobachtung lässt auf eine konsistentere, wenn auch suboptimale, Planungsstrategie schließen. Der beste erreichte Reward von 128,96 Punkten demonstriert das theoretische Lösungspotential des Szenarios, wird aber aufgrund der problematischen Exploration-Exploitation-Balance nicht stabil reproduziert. Die finale Exploration Rate von 0,108 liegt deutlich über dem optimalen Wert des erfolgreichen Szenarios B Alternative (0,067), was eine unzureichende Konvergenz zur Exploitation-Phase indiziert.

Der dramatische Performance-Unterschied von über 7.500 Punkten zwischen den Szenarien (964,20 vs. -6.543,79 Punkte) unterstreicht die Notwendigkeit szenario-spezifischer Algorithmus-Anpassungen und verdeutlicht die Sensitivität von Reinforcement Learning-Ansätzen gegenüber der Problemformulierung. Während Szenario B Alternative bereits nach 600 Episoden eine stabile Konvergenz erreicht, zeigt Szenario A selbst nach 2.000 Episoden keine Anzeichen einer Verbesserung der durchschnittlichen Performance.

Die vergleichende Analyse der Stabilitätskoeffizienten offenbart einen Faktor von 21,4 besserer Stabilität in Szenario B Alternative, was die unterschiedliche Qualität der Konvergenzprozesse quantifiziert. Diese Ergebnisse bestätigen die progressive Verbesserung der Planungsqualität während erfolgreicher RL-Trainingsphasen und identifizieren gleichzeitig kritische Faktoren für ausbleibende Konvergenz, insbesondere in der Gestaltung der Belohnungsfunktion und der Constraint-Struktur.

Die Erkenntnisse aus der Konvergenzanalyse haben direkte Implikationen für die praktische Anwendung von Reinforcement Learning in der Schichtplanung. Sie verdeutlichen, dass die theoretischen Vorteile von RL-Ansätzen nur dann realisiert werden können, wenn die Problemmodellierung sorgfältig an die spezifischen Charakteristika des Anwendungsfalls angepasst wird. Für zukünftige Implementierungen empfiehlt sich daher eine iterative Entwicklung der Belohnungsstruktur mit kontinuierlicher Überwachung der Konvergenz-Metriken.